# -*- coding: utf-8 -*-
"""liver _patient

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jEiJ4dVFgHDeDjQQu-YwOzz6JGNojnov
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

from google.colab import files
uploaded = files.upload()

#import the dataset from specified location 
data = pd.read_csv('indian_liver_patient (1).csv')

data.head()

data.info()

data.isnull().sum()

data['Dataset'].unique()

data['Albumin_and_Globulin_Ratio'] = data.fillna(data['Albumin_and_Globulin_Ratio'].mode()[0])
data.isnull().sum()

from sklearn.preprocessing import LabelEncoder
lc = LabelEncoder()
data['gender']= lc.fit_transform(data['gender'])

data.describe()

sns.displot(data['age'])
plt.title('Age Distribution Graph')
plt.show()

sns.countplot(data['outcome'], hue=data['gender'])

for i in cat12:
    print("Columns :",i)
    print(c(data[i]))
    print('*'*120+'\n')

catcols=set(data.dtypes[data.dtypes=='0'].index.values)
print(catcols)

pip install imblearn

from imblearn.over_sampling import SMOTE
smote = SMOTE()

y_train.value_count()

X_train_smote, y_train_smote = smote.fit_resample(X_train,y_train)

y_train_smote.value_counts()

from sklearn.ensemble import RandomForestClassifier
model1=RandomForeatClassifier()
model1.fit(X_train_smote, y_train_smote)
y_predict=model1.predict(X_test)
rfc1=accuracy_score(y_test,y_predict)
rfc1
pd.crosstab(y_test, y_predict)
print(classification_report(y_test, y_predict))

from sklearn.tree import DecisionTreeClassifier
model4=DecisionTreeClassifier()                      
model4.fit(X_train_smote, y_train_smote)
y_predict=model4.predict(X_test)
dtc1=accuracy_score(y_test,y_predict)
dtc1
pd.crosstab(y_test,y_predict)
print(classification_report(y_test,y_predict))

from sklearn.neighbors import KNeighborsClassifier
model2=KNeighborsClassifier()
model2.fit(X_train_smote, y_train_smote)
y_predict = model2.predict(X_test)
knn1=(accuracy_score(y_test, y_predict)
knn1
pd.crosstab(y_test,y_predict)
print(classification_report(y_test, y_predict))

from sklearn.linear_model import LogisticRegression
model5=LogisticRegression()
model5.fit(X_train_smote, y_train_smote)
y_predict=model5.predict(X_test)
logi1=accuracy_score(y_test, y_predict)
logi1
pd.crosstab(y_test,y_predict)
print(classification_report(y_test, y_predict))

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

classifier = Sequential()

classifier.add(Dense(units=100, activation='relu', input_dim=10))

classifier.add(Dence(units=50, activation='relu'))

classifier.add(Dence(units=1, activation='sigmoid'))

classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model_history = classifier.fit(X_train, y_train, batch_size=100, validation_split=0.2, epochs=100)

plt.figure(figsize=(10,7))

sns.heatmat(df.corr(),annot=True)

x=data.iloc[:,:-1]
y=data.outcome

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X_scaled,y, test_size=0.2, random_state=42)

from sklearn.preprocessing import scale
X_scaled=pd.DataFrame (scale(X), columns=X.columns)